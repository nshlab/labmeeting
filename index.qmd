---
title: "Fall 2025"
---

## Meeting Schedule

| Date            | Topic                  | Reading          | Presenter   |
|-----------------|------------------------|------------------|-------------|
| 8^th^ September  | tail and concentration bounds, uniform laws of large numbers | @wainwright2019highdim: Ch. 2, 4 | SB |
| 22^nd^ September     | metric entropy, minimax lower bounds | @wainwright2019highdim: Ch. 5, 15 | HL |
| 6^th^ October    | regularized regression and M-estimation | @wainwright2019highdim: Ch. 7, 9 | CT |
| 20^th^ October    | RKHS, non-parametric least squares | @wainwright2019highdim: Ch. 12, 13 | CJ |
| 3^rd^ November    |  |  |  |
| 17^th^ November    | |  |  |
| 1^st^ December (FXB G03)    | |  | |
| 15^th^ December    | |  | |

: We will meet biweekly **on Mondays**, 10:00AM-12:00PM, in HSPH 2-426.

In the upcoming Fall term, we will tentatively discuss various topics in
high-dimensional statistics, statistical learning theory and machine learning,
with specific material drawn from and references made to the following texts:
@wainwright2019highdim, @bickel2015mathematical, @hardt2022patterns,
@bach2024learning, and @duchi2024statistics. We will attempt to sample some of
the following:

* @wainwright2019highdim: Ch. 2 (tail and concentration bounds), 4
  (uniform laws of large numbers), 5 (metric entropy), 7 (sparse linear
  models), 9 (regularized M-estimators), 12 (RKHS, including kernel ridge
  regression), 13 (non-parametric least squares), 15 (minimax lower bounds)
* @bickel2015mathematical: Ch. 7 (tools for asymptotic analysis), 9 (inference
  in semi-parametric models), 12 (prediction and machine learning)
* @hardt2022patterns: Ch. 3 (supervised learning), 5 (optimization), 6
  (generalization), 7 (deep learning), 11 (sequential decision-making), 12
  (reinforcement learning)
* @duchi2024statistics: Ch. 2 (basics of information theory), 4 (concentration
  inequalities), 5 (generalization and stability), 8 (minimax lower bounds),
* @bach2024learning: Ch. 1 (mathematical preliminaries), 2 (supervised
  learning), 3 (linear least-squares), 4 (empirical risk minimization), 5
  (optimization for machine learning), 7 (kernel methods), 8 (sparse methods)

## References

::: {#refs}
:::
