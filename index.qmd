---
title: "Fall 2025"
---

## Meeting Schedule

| Date             | Room    | Topic                                                           | Reading                            | Presenter |
| ---------------- | ------- | --------------------------------------------------------------- | ---------------------------------- | --------- |
| 8^th^ September  | 2-426   | tail and concentration bounds, uniform laws of large numbers    | @wainwright2019highdim: Ch. 2, 4   | SB        |
| 22^nd^ September | 2-426   | metric entropy, minimax lower bounds                            | @wainwright2019highdim: Ch. 5, 15  | HL        |
| 6^th^ October    | 2-426   | regularized regression and M-estimation                         | @wainwright2019highdim: Ch. 7, 9   | CT        |
| 20^th^ October   | 2-426   | RKHS, non-parametric least squares                              | @wainwright2019highdim: Ch. 12, 13 | CJ        |
| 3^rd^ November   | 2-426   | supervised learning, representations and features, optimization | @hardt2022patterns Ch. 3, 4, 5     | NSH       |
| 17^th^ November  | 2-426   | generalization, deep learning                                   | @hardt2022patterns Ch. 6, 7        | CT        |
| 8^th^ December   | FXB-G03 | sequential decision-making, reinforcement learning              | @hardt2022patterns Ch. 11, 12      | SB        |

: We will meet biweekly **on Mondays**, 10:00AM-12:00PM, usually in HSPH 2-426.

In the Fall 2025 term, we will discuss topics in high-dimensional statistics
and statistical machine learning, with material drawn from the texts
@wainwright2019highdim and @hardt2022patterns (see details below). We will
continue along this theme in the Spring 2026 term.

- @wainwright2019highdim: Ch. 2 (tail and concentration bounds), 4
  (uniform laws of large numbers), 5 (metric entropy), 7 (sparse linear
  models), 9 (regularized M-estimators), 12 (RKHS, including kernel ridge
  regression), 13 (non-parametric least squares), 15 (minimax lower bounds)
- @hardt2022patterns: Ch. 3 (supervised learning), 4 (representation and
  features), 5 (optimization), 6 (generalization), 7 (deep learning), 11
  (sequential decision-making), 12 (reinforcement learning)

## References

::: {#refs}
:::
